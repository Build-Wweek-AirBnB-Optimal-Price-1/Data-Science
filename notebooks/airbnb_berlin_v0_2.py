# -*- coding: utf-8 -*-
"""AirBnB_Berlin_V0.2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WTsUNqr6eNVVVTk92-YE-wsAZD6MD5I8
"""

# Commented out IPython magic to ensure Python compatibility.
## Importing required libraries
import pandas as pd
import numpy as np
from numpy.random import seed
seed(14)
import matplotlib.pyplot as plt
# %matplotlib inline
from datetime import datetime
import seaborn as sns
##import geopandas as gpd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
import xgboost as xgb
from xgboost import plot_importance
from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score
import time
from keras import models, layers, optimizers, regularizers
from keras.utils.vis_utils import model_to_dot
from IPython.display import SVG
from statsmodels.tsa.seasonal import seasonal_decompose

## import the data
raw_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/listings.csv')
## print statement to show number of rows
print(f"The dataset contains {len(raw_df)} Airbnb listings")
## display all the columns
pd.set_option('display.max_columns', len(raw_df.columns)) 
## see 100 rows when looking at dataframe
pd.set_option('display.max_rows', 100)
## inspect the first three rows
raw_df.head(3)

## check number of columns
raw_df.shape

## drop columns that are non-numeric
cols_to_drop = ['listing_url', 'scrape_id', 'last_scraped', 'name', 'summary', 'space', 'description', 'neighborhood_overview', 'notes', 'transit', 'access', 'interaction', 'house_rules', 'thumbnail_url', 'medium_url', 'picture_url', 'xl_picture_url', 'host_id', 'host_url', 'host_name', 'host_location', 'host_about', 'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood', 'host_verifications', 'calendar_last_scraped']
df = raw_df.drop(cols_to_drop, axis=1)

## check for nan values
df.isna().sum()

## drop columns with large amounts of nans
df.drop(['host_acceptance_rate', 'neighbourhood_group_cleansed', 'square_feet', 'weekly_price', 'monthly_price', 'license', 'jurisdiction_names'], axis=1, inplace=True)

## set the index to be id 
df.set_index('id', inplace=True)

## drop highly correlated or repetitive  columns
df.drop(['host_total_listings_count', 'calculated_host_listings_count', 'calculated_host_listings_count_entire_homes', 'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms'], axis=1, inplace=True)

## save latitude and longitude data
lat_long = df[['latitude', 'longitude']]

## drop columns related to location since we are in Berlin exclusivly and we saved latitude and longitude 
df.drop(['zipcode', 'latitude', 'longitude', 'street', 'neighbourhood', 'city', 'state', 'market', 'smart_location', 'country_code', 'country', 'is_location_exact'], axis=1, inplace=True)

## drop columns highly correlated with nights/stayed
df.drop(['minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm'], axis=1, inplace=True)

df.head()

## Replacing columns with false/true with 0/1
df.replace({'f': 0, 't': 1}, inplace=True)

## Plotting the distribution of numerical and boolean categories
df.hist(figsize=(20,20));

## if there is only one category that is not helpful so we'll drop those columns
df.drop(['has_availability', 'host_has_profile_pic', 'is_business_travel_ready', 'require_guest_phone_verification', 'require_guest_profile_picture', 'requires_license'], axis=1, inplace=True)

## we have dropped a lot of non-essential columns
## now to clean the ones we have remaining
df.shape

## list remaining features
df.columns

"""## Experiences offered"""

## not useful 
df.experiences_offered.value_counts()

## drop experiences_offered columns since it was all none
df.drop('experiences_offered', axis=1, inplace=True)

"""## Host Since"""

### Converting to datetime
df.host_since = pd.to_datetime(df.host_since) 

## Calculating the number of days host has been active up to day data was scraped
df['host_days_active'] = (datetime(2019, 11, 12) - df.host_since).astype('timedelta64[D]')

## Printing mean and median
print("Mean days as host:", round(df['host_days_active'].mean(),0))
print("Median days as host:", df['host_days_active'].median())

#@ Replacing null values with the median
df.host_days_active.fillna(df.host_days_active.median(), inplace=True)

"""## Host response time"""

print("Null values:", df.host_response_time.isna().sum())
print(f"Percentage Null: {round((df.host_response_time.isna().sum()/len(df))*100, 1)}%")

# Number of rows without a value for host_response_time which have also not been reviewed
len(df[df.loc[ :,['host_response_time ', 'first_review'] ].isnull().sum(axis=1) == 2])

## we are going to make an unknown category for later use
df.host_response_time.fillna("unknown", inplace=True)
df.host_response_time.value_counts(normalize=True)

"""## Host response rate"""

## check percetage of null values for this category
print("Null values:", df.host_response_rate.isna().sum())
print(f"Percentage Null: {round((df.host_response_rate.isna().sum()/len(df))*100, 1)}%")

## Remove % sign from the host_response_rate string and converting to an integer for machine learning
df.host_response_rate = df.host_response_rate.str[:-1].astype('float64')
## inspect response rates
print("Mean host response rate:", round(df['host_response_rate'].mean(),0))
print("Median host response rate:", df['host_response_rate'].median())

## Categorize and bin response rate categories
df.host_response_rate = pd.cut(df.host_response_rate, bins=[0, 50, 90, 99, 100], labels=['0-49%', '50-89%', '90-99%', '100%'], include_lowest=True)

## Convert to string
df.host_response_rate = df.host_response_rate.astype('str')

## Replace nulls with 'unknown'
df.host_response_rate.replace('nan', 'unknown', inplace=True)

## Category counts
df.host_response_rate.value_counts()

"""## superhost"""

## find the number of instances where superhost is null
## Number of rows without a value for multiple host-related columns
len(df[df.loc[ :,['host_since ', 'host_is_superhost', 'host_listings_count', 'host_has_profile_pic', 'host_identity_verified'] ].isnull().sum(axis=1) == 5])

## drop rows where multiple items are missing
df.dropna(subset=['host_since'], inplace=True)

"""## property type"""

## inspect feature
df.property_type.value_counts()

## Categorize properties as house or apartment
df.property_type.replace({
    'Condominium' : 'House',
    'Loft' : 'Apartment',
    'Serviced apartment': 'Apartment',
    'Townhouse' : 'House',
    'Hotel' : 'Apartment',
    'Boutique hotel' : 'Apartment',
    'Guest suite' : 'Apartment',
    'Guesthouse' : 'House',
    'Bungalow': 'House',
    'Houseboat' : 'House',
    'Villa' : 'House',
    'Aparthotel' : 'Apartment',
    'Tiny house': 'House',
    'Cottage' : 'House',
    'Treehouse' : 'House',
    'Castle' : 'House',
    'Lighthouse' : 'House',
    'Earth house': 'House'  
    }, inplace=True)

## Replace all the other categories with category type other
## if its not an apartment or a house its other
df.loc[~df.property_type.isin(['House', 'Apartment']), 'property_type'] = 'Other'

## inspect our work
df.property_type.value_counts()

"""## bathrooms, bedrooms and beds"""

## for loop that will go through these three columns
## it will fill the median value for each column
## we are using median values so we don't have .73 bedrooms or something wierd like that
for col in ['bathrooms', 'bedrooms', 'beds']:
    df[col].fillna(df[col].median(), inplace=True)

"""## bed type"""

## inspect values
df.bed_type.value_counts(normalize=True)

## since 97% have the same value this feature will be dropped
df.drop('bed_type', axis=1, inplace=True)

"""## amenities"""

## inspect amenities for a listing
# Example of amenities listed
df.amenities[:1].values

# Creating a set of all possible unique amenities
## cast the array as list
amenities_list = list(df.amenities)
## join all the words together using empty space
amenities_list_string = " ".join(amenities_list)
## replace open brackets with empty space
amenities_list_string = amenities_list_string.replace('{', '')
## replace closing brackets with comma
amenities_list_string = amenities_list_string.replace('}', ',')
## remove extra punctuation
amenities_list_string = amenities_list_string.replace('"', '')
## strip whitespace and split on commas
amenities_set = [x.strip() for x in amenities_list_string.split(',')]
## remove duplicates
amenities_set = set(amenities_set)
## inspect our work
amenities_set

## we can bin many of these amenities together

## change 24 hour check in to a better format
df.loc[df['amenities'].str.contains('24-hour check-in'), 'check_in_24h'] = 1

## handicap accessible 
df.loc[df['amenities'].str.contains('Accessible-height bed|Flat path to guest entrance|Step-free shower|No stairs or steps to enter|Fixed grab bars for toilet|Fixed grab bars for shower|Accessible-height toilet|Wide entryway|Wide hallways|Wide entrance for guests|Wide entrance|Wide doorway to guest bathroom|Wide clearance to shower|Disabled parking spot'), 'handicap_accessible'] = 1

## add category for high_end_electronics
df.loc[df['amenities'].str.contains('Amazon Echo|HBO GO|High-resolution computer monitor|Sound system|Printer|Laptop friendly workspace|DVD player|Game console|Netflix|Projector and screen|Smart TV'), 'high_end_electronics'] = 1

## add category for air conditioning
df.loc[df['amenities'].str.contains('Air conditioning|Heating'), 'air_conditioning'] = 1

## add category for general appliances
df.loc[df['amenities'].str.contains('Convection oven|Essentials|Stove|Refrigerator|Oven|Microwave|Gas oven|Double oven|Dishwasher'), 'standard_appliances'] = 1

## add category for high end appliances
df.loc[df['amenities'].str.contains('Air purifier|Wine cooler|Mobile hoist|Mini fridge|Steam oven|Room-darkening shades|Indoor fireplace|Hot water kettle|Heated floors|Electric profiling bed|Ceiling hoist|Ceiling fan|'), 'high_end_appliances'] = 1

## add category for standard bath amenities
df.loc[df['amenities'].str.contains('Bath towel|Hot water|toilet|Walk-in shower|Toilet paper|Shampoo|Hair dryer|Soaking tub|Body soap|Bathtub|Bathroom essentials'), 'standard_bathroom_items'] = 1

## add category for high end bath amenities
df.loc[df['amenities'].str.contains('Bathtub with bath chair|En suite bathroom|Warming drawer|Stand alone steam shower|Shower chair|Rain shower|Heated towel rack|Handheld shower head|Bidet'), 'bathroom_items_luxury'] = 1

## add category for laundry essentials
df.loc[df['amenities'].str.contains('Dryer|Washer / Dryer|Washer|Hangers|Iron'), 'laundry_essentials'] = 1

## add category for kitchen essentials
df.loc[df['amenities'].str.contains('Cooking basics|Kitchenette|Dishes and silverware'), 'kitchen_essentials'] = 1

## add category for high end kitchen items
df.loc[df['amenities'].str.contains('Breakfast table|Formal dining area|Kitchen|Full kitchen'), 'kitchen_luxury'] = 1

## add category for kid friendly
df.loc[df['amenities'].str.contains('Baby bath|Stair gates|Pack ’n Play/travel crib|High chair|Family/kid friendly|Crib|Children’s dinnerware|Children’s books and toys|Changing table|Babysitter recommendations|Baby monitor'), 'child_friendly'] = 1

## add category for safety and security
df.loc[df['amenities'].str.contains('Buzzer/wireless intercom|Well-lit path to entrance|Window guards|Table corner guards|Smoke detector|Smart lock|Safety card|Outlet covers|Lockbox|Lock on bedroom door|Keypad|First aid kit|Fireplace guards|Fire extinguisher|Carbon monoxide detector'), 'safety_and_security'] = 1

## add category for outdoor space
df.loc[df['amenities'].str.contains('Balcony|Sun loungers|Terrace|Patio or balcony|Hammock|Outdoor seating|Mudroom|Fire pit|Garden or backyard'), 'outdoor_space'] = 1

## add category for bbq
df.loc[df['amenities'].str.contains('BBQ grill'), 'bbq'] = 1

## add category for natural amenities
df.loc[df['amenities'].str.contains('Beach view|Beach essentials|Beachfront|Lake access|Mountain view|Ski-in/Ski-out|Waterfront'), 'nature_and_views'] = 1

## add category for tv
df.loc[df['amenities'].str.contains('TV|Cable TV'), 'tv'] = 1

## add category for pets
df.loc[df['amenities'].str.contains('Pets|Pets live on this property|Pets allowed|Other pet(s)|pet|Cat(s)|Dog(s)'), 'pets_allowed'] = 1

## add category for bedding essentials
df.loc[df['amenities'].str.contains('Bed linens|Firm mattress'), 'bedding_essentials'] = 1

## add category for bedding essentials
df.loc[df['amenities'].str.contains('Bedroom comforts|Pillow-top mattress|Murphy bed|Memory foam mattress|Extra space around bed|Day bed|Extra pillows and blankets'), 'bedding_luxury'] = 1

## add category for coffee machines
df.loc[df['amenities'].str.contains('Coffee maker|Espresso machine'), 'coffee_machine'] = 1

## add category for included breakfast
df.loc[df['amenities'].str.contains('Breakfast'), 'breakfast'] = 1

## add category for internet access
df.loc[df['amenities'].str.contains('Internet|Ethernet connection|Pocket wifi|Wifi'), 'internet'] = 1

## add category for onsite staff
df.loc[df['amenities'].str.contains('Building staff|Standing valet|Doorman'), 'onsite_staff'] = 1

## add category for cleaning before checkout
df.loc[df['amenities'].str.contains('Cleaning before checkout'), 'clean_before_checkout'] = 1

## add category for EV charger
df.loc[df['amenities'].str.contains('EV charger'), 'ev_charger'] = 1

## add category for elevator
df.loc[df['amenities'].str.contains('Elevator'), 'elevator'] = 1

## add category for free parking
df.loc[df['amenities'].str.contains('Free parking on premises|Outdoor parking|Free street parking'), 'free_parking'] = 1

## add category for ground floor 
df.loc[df['amenities'].str.contains('Ground floor access|Single level home'), 'ground_floor'] = 1

## add category for gym
df.loc[df['amenities'].str.contains('Gym'), 'gym'] = 1

## add category for host greets you
df.loc[df['amenities'].str.contains('Host greets you'), 'host_greeting'] = 1

## add category for host greets you
df.loc[df['amenities'].str.contains('Host greets you'), 'host_greeting'] = 1

## add category for hot tub or pool
df.loc[df['amenities'].str.contains('Hot tub|Pool|Pool with pool hoist'), 'hot_tub_or_pool'] = 1

## add category for host greets you
df.loc[df['amenities'].str.contains('Long term stays allowed'), 'long_term_stays'] = 1

## add category for host greets you
df.loc[df['amenities'].str.contains('Luggage dropoff allowed'), 'luggage_dropoff'] = 1

## add category for other
df.loc[df['amenities'].str.contains('Other|translation missing: en.hosting_amenity_50|translation missing: en.hosting_amenity_49'), 'other'] = 1

## add category for paid parking
df.loc[df['amenities'].str.contains('Paid parking off premises|Paid parking on premises'), 'paid_parking'] = 1

## add category for privacy 
df.loc[df['amenities'].str.contains('Private bathroom|Private entrance|Private hot tub|Private living room'), 'privacy'] = 1

## add category for self check in
df.loc[df['amenities'].str.contains('Self check-in'), 'self_check_in'] = 1

## add category for host greets you
df.loc[df['amenities'].str.contains('Smoking allowed'), 'smoking_allowed'] = 1

## add category for host greets you
df.loc[df['amenities'].str.contains('Suitable for events'), 'suitable_for_events'] = 1

## reduce dimensionality by dropping infrequent amenities
# Replacing nulls with zeros for new columns
cols_to_replace_nulls = df.iloc[:,41:].columns
df[cols_to_replace_nulls] = df[cols_to_replace_nulls].fillna(0)

# Produces a list of amenity features where one category (true or false) contains fewer than 10% of listings
infrequent_amenities = []
for col in df.iloc[:,41:].columns:
    if df[col].sum() < len(df)/10:
        infrequent_amenities.append(col)
print(infrequent_amenities)

# Dropping infrequent amenity features
df.drop(infrequent_amenities, axis=1, inplace=True)

# Dropping the original amenity feature
df.drop('amenities', axis=1, inplace=True)

## inspect frequently occuring amenities
df.columns[40:]

"""## price"""

## for price we need to convert it from string to integer and remove money sign
df.price = df.price.str[1:-3]
df.price = df.price.str.replace(",", "")
df.price = df.price.astype('int64')

## inspect changes
df.price

"""## security deposit"""

## convert this string into an integer 
## remove currency symbol
## fill empty values with 0 meaning no security deposit
df.security_deposit = df.security_deposit.str[1:-3]
df.security_deposit = df.security_deposit.str.replace(",", "")
df.security_deposit.fillna(0, inplace=True)
df.security_deposit = df.security_deposit.astype('int64')

"""## cleaning fee"""

## same as above
df.cleaning_fee = df.cleaning_fee.str[1:-3]
df.cleaning_fee = df.cleaning_fee.str.replace(",", "")
df.cleaning_fee.fillna(0, inplace=True)
df.cleaning_fee = df.cleaning_fee.astype('int64')

"""## extra people"""

## same as above
df.extra_people = df.extra_people.str[1:-3]
df.extra_people = df.extra_people.str.replace(",", "")
df.extra_people.fillna(0, inplace=True)
df.extra_people = df.extra_people.astype('int64')

"""## calender updated"""

## check number of categories
df.calendar_updated.nunique()

df.calendar_updated.value_counts(normalize=True)

## too much variation, this feature will be dropped
df.drop('calendar_updated', axis=1, inplace=True)

"""## availability"""

## more than 90 days is no longer short term rental so we will drop more than 90 days
df.drop(['availability_30', 'availability_60', 'availability_365'], axis=1, inplace=True)

"""## first review last review"""

df.first_review = pd.to_datetime(df.first_review) # Converting to datetime

# Calculating the number of days between the first review and the date the data was scraped
df['time_since_first_review'] = (datetime(2019, 11, 12) - df.first_review).astype('timedelta64[D]')

## function to categorize review duration
def bin_column(col, bins, labels, na_label='unknown'):
    """
    Takes in a column name, bin cut points and labels, replaces the original column with a
    binned version, and replaces nulls (with 'unknown' if unspecified).
    """
    df[col] = pd.cut(df[col], bins=bins, labels=labels, include_lowest=True)
    df[col] = df[col].astype('str')
    df[col].fillna(na_label, inplace=True)

## Binning time since first review
bin_column('time_since_first_review',
           bins=[0, 182, 365, 730, 1460, max(df.time_since_first_review)],
           labels=['0-6 months',
                   '6-12 months',
                   '1-2 years',
                   '2-3 years',
                   '4+ years'],
           na_label='no reviews')

## repeat the process for last review
df.last_review = pd.to_datetime(df.last_review) # Converting to datetime

# Calculating the number of days between the most recent review and the date the data was scraped
df['time_since_last_review'] = (datetime(2019, 11, 12) - df.last_review).astype('timedelta64[D]')

# Binning time since last review
bin_column('time_since_last_review',
           bins=[0, 14, 60, 182, 365, max(df.time_since_last_review)],
           labels=['0-2 weeks',
                   '2-8 weeks',
                   '2-6 months',
                   '6-12 months',
                   '1+ year'],
           na_label='no reviews')

## Dropping last_review - first_review will be kept for EDA and dropped later
df.drop('last_review', axis=1, inplace=True)

"""## review ratings columns"""

## rooms with no review will be unknown
## others will be binned
## visualize distributions
# Checking the distributions of the review ratings columns
variables_to_plot = list(df.columns[df.columns.str.startswith("review_scores") == True])
fig = plt.figure(figsize=(12,8))
for i, var_name in enumerate(variables_to_plot):
    ax = fig.add_subplot(3,3,i+1)
    df[var_name].hist(bins=10,ax=ax)
    ax.set_title(var_name)
fig.tight_layout()
plt.show()

## remove review_scores_rating because it is out of 100 not 10 like the others
variables_to_plot.pop(0)

## Binning for all columns scored out of 10
for col in variables_to_plot:
    bin_column(col,
               bins=[0, 8, 9, 10],
               labels=['0-8/10', '9/10', '10/10'],
               na_label='no reviews')

## Binning column scored out of 100
## dividing by 100 to match other ratings categories
bin_column('review_scores_rating',
           bins=[0, 80, 95, 100],
           labels=['0-79/100', '80-94/100', '95-100/100'],
           na_label='no reviews')

"""## Cancellation policy"""

## view value counts
df.cancellation_policy.value_counts()

## place stricts all into the same category
# Replacing categories
df.cancellation_policy.replace({
    'super_strict_30': 'strict_14_with_grace_period',
    'super_strict_60': 'strict_14_with_grace_period',
    'strict': 'strict_14_with_grace_period',
    }, inplace=True)

## inspect changes
df.cancellation_policy.value_counts(normalize=True)

"""## number_of_reviews_ltm and reviews_per_month"""

## These will be highly correlated with number_of_reviews and so will be dropped
df.drop(['number_of_reviews_ltm', 'reviews_per_month'], axis=1, inplace=True)

## inspect dataframe
df.head()

df.shape

## Dropping columns that are not useful to price analysis
df.drop(['host_since', 'first_review'], axis=1, inplace=True)

"""# Numerical features"""

df.describe()

print(min(df.price), max(df.price))

### Distribution of prices from $0 to $1000
plt.figure(figsize=(20,4))
df.price.hist(bins=100, range=(0,1000))
plt.margins(x=0)
plt.title("Airbnb advertised nightly prices in Berlin up to $1000", fontsize=16)
plt.xlabel("Price ($)")
plt.ylabel("Number of listings")
plt.show()

## replace outlier values 

# Replacing values under £10 with £10
df.loc[df.price <= 10, 'price'] = 10

# Replacing values over £1000 with £1000
df.loc[df.price >= 1000, 'price'] = 1000



"""## Getting data ready for machine learning"""

## one hot encoding
transformed_df = pd.get_dummies(df)

## Dropping collinear features
to_drop = ['beds',
           'guests_included', 
           'host_response_rate_unknown',
           'host_response_rate_0-49%',
           'property_type_Apartment',
           'room_type_Private room']
to_drop.extend(list(transformed_df.columns[transformed_df.columns.str.endswith('nan')]))

transformed_df.drop(to_drop, axis=1, inplace=True)

numerical_columns = ['accommodates', 'availability_90', 'bedrooms', 'bathrooms', 'cleaning_fee', 'extra_people', 'host_days_active', 'host_listings_count', 'maximum_nights', 'minimum_nights', 'number_of_reviews', 'price', 'security_deposit']

## Log transforming columns
numerical_columns = [i for i in numerical_columns if i not in ['availability_90', 'host_days_active']] # Removing items not to be transformed

for col in numerical_columns:
    transformed_df[col] = transformed_df[col].astype('float64').replace(0.0, 0.01) # Replacing 0s with 0.01
    transformed_df[col] = np.log(transformed_df[col])

## split into features matrix and target vector

# Separating X and y
X = transformed_df.drop('price', axis=1)
y = transformed_df.price

# Scaling
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=list(X.columns))

"""# Neural Network to predict Price"""

## train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=14)

## baseline XGboost machien learning model

xgb_reg_start = time.time()

xgb_reg = xgb.XGBRegressor()
xgb_reg.fit(X_train, y_train)
training_preds_xgb_reg = xgb_reg.predict(X_train)
val_preds_xgb_reg = xgb_reg.predict(X_test)

xgb_reg_end = time.time()

print(f"Time taken to run: {round((xgb_reg_end - xgb_reg_start)/60,1)} minutes")
print("\nTraining MSE:", round(mean_squared_error(y_train, training_preds_xgb_reg),4))
print("Validation MSE:", round(mean_squared_error(y_test, val_preds_xgb_reg),4))
print("\nTraining r2:", round(r2_score(y_train, training_preds_xgb_reg),4))
print("Validation r2:", round(r2_score(y_test, val_preds_xgb_reg),4))

with open('xgb_reg_pickle', 'wb') as f:
  pickle.dump(xgb_reg, f)

## plot feature importances
ft_weights_xgb_reg = pd.DataFrame(xgb_reg.feature_importances_, columns=['weight'], index=X_train.columns)
ft_weights_xgb_reg.sort_values('weight', inplace=True)
ft_weights_xgb_reg

## Plotting feature importances
plt.figure(figsize=(8,20))
plt.barh(ft_weights_xgb_reg.index, ft_weights_xgb_reg.weight, align='center') 
plt.title("Feature importances in the XGBoost model", fontsize=14)
plt.xlabel("Feature importance")
plt.margins(y=0.01)
plt.show()

# Building the model
nn = models.Sequential()
nn.add(layers.Dense(128, input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l1(0.005), activation='relu'))
nn.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.005), activation='relu'))
nn.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.005), activation='relu'))
nn.add(layers.Dense(512, kernel_regularizer=regularizers.l1(0.005), activation='relu'))
nn.add(layers.Dense(1, activation='linear'))

# Compiling the model
nn.compile(loss='mean_squared_error',
            optimizer='adam',
            metrics=['mean_squared_error'])

# Model summary
print(nn.summary())

# Training the model
history = nn.fit(X_train,
                  y_train,
                  epochs=150,
                  batch_size=256,
                  validation_split = 0.15)

"""## Save model to send to data engineers"""

pwd

import pickle

## commenting out so doesn't resave when running again
'''
with open('nn_model_pickle', 'wb') as f:
  pickle.dump(nn, f)
'''

## commenting out so doesn't resave when running again
'''
with open('processed_df_pickle', 'wb') as pickle_file:
  pickle.dump(transformed_df, pickle_file)
'''

transformed_df

pwd

transformed_df.to_csv('/content/transformed_df.csv',index=False)

